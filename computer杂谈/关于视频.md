so many Pdfs_Refs：[video_codec_learn（fork from codec2021）](https://github.com/waaall/video_codec_learn/tree/main)
# 前言
分辨率，位深，色域，色度采样，编码格式，封装格式等等，这篇文章试着讲清楚。但是这篇文章需要一定的显示技术的基础知识，若从最基本的显示技术，比如如何用三原色混合显示出其他颜色；再比如传统LCD显示器如何控制背光、液晶层，有何缺点？miniLED显示器的分区背光、OLED显示器有何优缺点等等。这些可以去看一些科普文章，其中硬件茶谈就做的不错，但也很浅显，适合小白。

首先，一个视频：看着清晰且几乎没有噪点、艳丽又不过饱和、明亮又不会过爆、暗的深邃又不会看不清，颜色过渡自然没有断层；那么对于同样的内容，看起来舒服太多了。
## 一个视频的“一生”
一个视频（除非是纯电脑制作的动画，但即使电脑制作的动画多数参数也遵循）一般是先经过**拍摄**，再经过**剪辑**，然后**导出**。若是网络视频，还需要经过**上传**，最终我们在设备上下载或者在线**播放**，通过**显示器**呈现出来。

这个过程的描述好像是废话，但我想强调的是，这是一个串联的过程，类似我之前讲过的网络，每一个中间环节“慢“了，都会直接影响最终效果，而多个过程出问题，会以乘积的量级对最终效果产生影响。

我决定倒着讲述这其中的原理和现象，因为绝大多数人都是由看视频开始了解视频，而不是拍视频，我也一样。
## 概述
一个视频有很多参数，几乎都会影响到我们最终观看的质量，而这些参数都会在下文中细致的讲到，但我还要强调这需要一定的基础，不仅仅是因为我很难在任何专有名词提出的时候都紧跟着大量篇幅的讲解说明，更是这种专有名词随便在网上一搜都会有很细致、生动的解释。还有一些比如延迟这种参数和看视频关系不大，主要是打FPS游戏需要，所以就没提。我想写这篇文章主要是两点：一是大致梳理整个高质量视频的产出到观看流程；二是重点讨论一些网络上搜到大量低质量内容或者根本搜不到答案的问题，比如一些参数的关系、编码哪些是标准哪些；然后结合这个流程中的理论知识，运用到看视频时可能会遇到的问题中去，比如**同样是4k为何有的清楚有的不清楚？同样HDR有的发灰有的过爆？同样广色域有的过饱和有的发灰？**

OLED 480hz   安卓：1920hz成本低，显示不好 ，现在多数旗舰手机 ，每个像素点三个灯
 
miniled 比oled更贵，但是显示效果中间，有点寿命高
LCD 大部分以前的手机、电脑、电视，成本最低，显示效果更差， 整个屏幕一个灯，
# 1显示器
## 1.1分辨率&刷新率
在很多年前（当然对于现在很多人的显示器还是老旧的）显示器素质很低时，分辨率和刷新率的短板是对与提高观看体验的短板，所以这也是显示器购买时最看重的，不谈尺寸和观看距离谈分辨率就是耍流氓，所以PPI和观看距离才是对分辨率的正确要求。这就要说到苹果的Retina标准了，但是也不是什么高深的，只不过没有其他厂商提出类似的标准。（PPI、Retina标准是什么自行搜索）
分辨率接近Retina标准、刷新率120Hz之后，这部分参数的短板就几乎不存在了。Retina标准如果不了解，完全可以参考苹果设备的分辨率，比如屏幕16寸3K多、24寸4K多，27寸5K，32寸6K等，我是指接近，比如27寸4K就差不多了，但是27寸6K或者8K就没有必要，除非下面所说参数全部补齐之后。

要注意不同设备，不同材料的屏幕对PPI的要求不同，比如OLED因为需要次像素渲染，实际体验相同的话，对PPI要求更高；再比如VR设备，PPI尤其是相比手机电脑来比较的意义不大，什么单眼4K的体验其实低于手机的1080P屏幕（这里只是说屏幕的观看清晰度体验，而VR设备特有的沉浸体验当然是另一回事）
刷新率其实也有一些值得说的，但是其对观影体验影响甚小，不在此文章的讨论范围。
## 1.2[色域](https://www.bilibili.com/video/BV1kk4y167rk/)和[色深](https://www.bilibili.com/video/BV1dp4y1S7ow/)
色深和色域没有直接的联系，**色域是量程，色深是分辨率**。（色域也不是“完全”的量程，因为量程有两个维度：一个是亮度的量程，另一个是波长的量程。而色域指的是波长的量程）
色深衡量色彩数量的多少，色深指的是rgb通道每个通道的颜色的灰度值，也是是亮度信息，可以用y表示，比如10bit也就是每个通道的亮度信息是10的二次方为1024，然后三个1024相乘就是可以显示的颜色，目前的一些高端显示器就是10bit的，色深越多，画面色彩过渡越平衡。当然，这里还有一些如8bit“抖动”10bit的，原理就类似**PWM调光**，上文链接的视频中解释的很清楚了，这里就不赘述了。
色彩空间简称色域，及颜色的覆盖范围，简单理解就是红可以多红，蓝可以多蓝，绿可以多绿？而显示中人眼所能看见的也是有限的（下图中全部的彩色范围），人眼更敏感能明显区分的也是有限的，另一方面，显示器技术所能准确显示的范围也是有限的。所以，它需要一种标准来衡量它。从sRGB(如下图所示)到DCI-P3，再现在的bt2020。当然还有AdobeRGB。

![](关于视频.assets/Cie_Chart_with_sRGB_gamut.png)

## 1.3亮度、对比度、HDR
亮度的范围就是对比度，亮度的精度就是色深；**亮度可以理解为上图中增加了一个Z轴**，Z轴为你把显示器的亮度由最低调整至最高。(下图引用自《索尼中国4K超高清HDR技术简介》)
![](关于视频.assets/色彩空间-包含亮度.png)
HDR要复杂得多，HDR不是一个独立于上述其他参数的标准，而是基于亮度对比度和色域色深提出的一系列综合性的“高”要求。顾名思义，也就是High Dynamic Range---高动态范围，既包括亮度范围和精度（亮度、色深），也包括色域，即图像或视频（视频本质就是多张图像）在光的高波长范围精度&高亮度范围和精度。
但是比如HDR10有很多推荐配置而不是硬性要求，比如显示器亮度1000nt就不是硬性要求，所以导致很多厂商利用这些点虚假宣传，反而会导致体验变差。杜比视界相比HDR10就好了很多，但是要求很高，专利授权费很高，但还是在不断普及。
## 1.4亮度和色域、色深的关系？
上述对亮度“为上图增加一个z轴”的粗暴描述是非常不准确的，而大部分科普文章也都会分开讨论色域（色彩空间）和亮度的关系，搜索其关系，也经常碰到说两者独立的描述。有个知乎回答，说的不太对，还是附在这里：[这些参数的区别？联系？](https://www.zhihu.com/question/342129041/answer/825753921)

因为，色彩空间中所有除RGB三原色之外的颜色，都是这三种颜色按照一定比例的**亮度**混合而成的。也就是说，比如在同样显示器亮度下的紫色和红色的亮度是完全不同的。见下图（来自维基百科）：
![8bit16色数据](关于视频.assets/8bit16色数据.png)
> Q: 如何能看起来让紫色和红色一样的亮度？但问题就在于，需要一样的亮度吗？
1. 因为人眼之所以看到不同颜色，就是三种视锥细胞感受到三种颜色（也就是仅对三种不同波长的光产生生理性的电化学反应），最终在大脑生成的“五颜六色”。而这三种视锥细胞几乎就对应的显示器的三原色（一是没有那么精准的对应；二是人对不同颜色的感觉敏感程度也不是线性叠加的，但是我们的期望是线性的，所以这种偏差需要厂商的精准调校）。
2. 显示器就是要模拟现实世界，而现实世界的绝大多数可见物体都是不发光的，而且是太阳这一全波长（近似）白光光源下吸收了其他的颜色，只有剩下波长的颜色反射了出来，所以白色就是比紫色要亮！so，这种现象和生理结构让显示器模拟现实世界的颜色时大大降低了难度。

> Q: 比如一个8bit的面板，即RGB亮度可调等级有256种(2^8)，但是如果我把显示器调到最低亮度，感觉还是可以准确显示其他颜色，所以这个可调等级是根据最低亮度调整的吗？如果是这样，比如100nt是可以8bit，200nt岂不可以9bit，400nt可以10bit？

前半句是对的，所以显示器的最低亮度并不会特别暗（暗到这个亮度下无法处理成256个等级），但后半句不对，理论上可行，但是这种亮度等级不是这么简单可以实现的：
比如LCD显示器，其实对RGB的亮度控制来实现不同颜色是通过液晶层的偏转实现的，而亮度是通过背光层发光材料的亮度实现，所以这么看来，位深是液晶层偏转角度的精细度，而亮度相对独立。但是OLED是自发光，反而不是独立控制了，更容易实现上述理论操作，但可能出于面板素质均一性考量？对此我就没有深究了。另外值得一提的是，虽然miniLED显示器在原理上仍归属于LCD显示器，但是其由于大量分区背光的特性，再显示原理上其实同时OLED和LCD，所以调试难度增加很多，因为需要控制好分区背光和液晶层的延迟等问题。
## However
但，即使你有一个很好的片源（比如4K HDR P3）和一个很好的显示器（同样4K HDR P3），依然很有可能得不到一个好的观看体验，因为这中间，还有两大步：
**显示器的这些参数仅是众多必要条件之一，播放器软件的适配性、操作系统的色彩管理和硬件素质决定着一个“好的”片源能否在好的显示器上正确的显示出来。**
# 2操作系统
倒着讲应该先讲播放器？但是显示器紧接着涉及到色彩管理，而编码格式可能也会涉及到硬件解码器、算法等计算机系统知识，so，就先讲这部分吧。现代操作系统非常复杂，这里仅说关于视频播放相关的一个重要事项---[色彩管理](https://www.bilibili.com/video/BV1wC4y1p7Xr/)，就结合这部分把上面显示器参数缺失的一个---色准，补上。

1. 就单论sRGB色域的SDR显示器显示准确性来讲，如何较为准确的显示白光？（白光理论上可以用255.255.255来表示，但实际上RGB的比例不会是1：1：1，甚至因为不同面板不同发光材料的不同，这种真实的亮度比例也会变化）如何利用线性代数计算颜色比例？见：[电子显示Display基础知识](https://blog.csdn.net/sy20173081277/article/details/117017809)。当然可能会以为这应该是显示器面板厂商的工作，但这就像是芯片，需要操作系统厂商和硬件厂商的深度合作才能完成。

2. 另外，因为内容如图片视频，会用不同的标准制作出来，比如一张P3色域HDR照片，如何比较准确的显示在sRGB的显示器上？一张sRGB的照片，如何准确的显示在P3色域的显示器上？也就是映射关系。理论很好理解，apple一直做的很好，安卓手机最近两年也变好了不少（2020年之前包括三星还是做的很烂），但现在2024年windows还是shit。apple在面板上并没有芯片这种设计研发能力，但是其与供应链深度合作共同研发的战略意识值得其他尤其是国产厂商学习。（国产厂商大多数闭关锁“厂”，生怕合作泄漏了自己的技术，然而自己还没多少技术，实在令人尴尬，甚至一个公司内部部门间合作亦如此）

这部分研究的很浅，之后有时间和兴趣再深入吧。
# 3[编解码与格式](https://www.bilibili.com/video/BV1ws41157f8/)
任何数据都需要有格式，只有按照一个规则排列，再按照一个规则展开，才能保证信息的准确，正如python文件是.py，c语言文件是.c等等，但，视频为何有各种各样的格式？
tim很多年前做的视频（标题链接），还有其他一搜就有的科普文章还不错的说明了这个问题，但我还是想要展开讨论一些细节。
## 编码
一个视频就是一组图片，而一张图片，就是一堆像素组成的方块（矩阵），而一个像素，由三原色的亮度信息组成，也就是RGB的亮度。
比如对于8bit位深的图片，一个像素就是24bit信息即3B（字节），而4k的1:1图片就是4000×4000个像素，也就是48MB，而对于4K60帧的视频来说，一秒就是60张4K图片，也就是1s的4K视频大约2GB。那我们是怎么看4K的？
一个图片中，很多信息都是重复的，比如蓝天都是差不多的，一个纯白色T恤也都是类似的信息，所以我们如果把类似的信息都用一份存储，剩下的只存储他们的位置（或者编好顺序存序号），虽然实际的算法还是很复杂，但是图片就可以被这样压缩了。这种被称为“帧内压缩”。
一个视频就是一堆图片，而比如60帧的视频，1s60张图片，而相邻图片的很多信息都是重复的，如果我们对于这种重复信息采用类似帧内编码的压缩算法，同样可以节省大量信息。
在像素级别其实就已经在“压缩”了，即色度采样。同样的道理，相邻像素大概率是重复的，所以干脆直接隔一个保存一个得了（当然不是这么简单粗暴的），比如对于不“压缩的”就是4:4:4采样，而很多视频都是4:2:0采样，这点标题链接中tim的视频也讲到了。所以，实际的图像也不是RGB来描述像素信息的，而是YUV，具体可以查维基百科。
具体压缩的算法和压缩后视频的格式是有明确的标准规定的（否则不乱套了，一家设备智能看自己公司制作的视频？），目前的主流是H256（HEVC）和AV1，我记得在2016年前后大多数视频还是H264的。H265相比H264的压缩算法可以在相同的画质下明显降低视频大小，当然具体如何实现，这就是数据结构和算法了，有兴趣可以参考论文：《Overview_of_the_High_Efficiency_Video_Coding_HEVC_Standard》。
> 该算法首先对视频的每帧图片进行分块，不同于H264的是，H265(HEVC)是可变分块，这样对于大面积相似像素的处理，是大大提高了压缩效率（以前还需要分好几块，现在一块一起描述，下图引用自《新一代高效视频编码H.265/HEVC:原理,标准与实现》）。
> ![CU四叉树](关于视频.assets/CU四叉树.png)
> 对于帧内编码，也就是对相邻块的像素进行“预测”（大概率相邻像素是很像的），与H264不同的是，H265提供了更多的预测模式，比如不仅对水平、垂直方向的块比较，还有不同角度的块。
> 对于帧间编码，也就是对相邻帧间的块的运动进行预测。一个不太准确的例子：视频中一个人挥了挥手，那对于手所在的块，相邻几帧进行了平移和旋转运动，这样的话，通过线性代数中学到的线性变换，就可以预测接下来几帧手所在的块，这样手这个块只需要存一帧，剩下的只需要存线性变换的参数就可以了）。但实际上运动预测是非常困难的，这也是编码算法中最耗时的，有些电视宣传的插帧技术，其实原理也是类似的，但是现在很多基于深度学习的插帧和画质增强技术就是另一码事了。
## 解码器
一方面，编码算法非常复杂，虽然解码要简单的多（因为不需要预测算法等），但还是较为复杂；另一方面，解码要求实时完成（因为看视频的时候边解码边播放），提前解码需要大量内存和硬盘空间，所以不现实。但是当今看视频又是电子产品的一个必需且占比非常大的功能，所以对硬件计算能力要求很高。“没有啊，看视频不卡也不热啊？”---正因为视频编解码的算法是相对固定的，所以芯片厂商在设计芯片是，直接针对这个算法设计硬件，实现高效率执行，也被称为硬件视频解码器，而调用硬件视频解码器看视频也被称为“硬解”，相反，CPU直接运行解码算法被称为“软解”。而高清视频软解对CPU性能要求很高，但是CPU软解几乎不会遇到兼容性问题（2024年H265硬解也基本遇不到啥大问题了）
## 码率
由上文可以得知，视频编码就是本质就是压缩，且是有损压缩，有些所谓科普文章认为两个概念是独立的，甚至说视频编码是无损的之类的，都是胡扯。类似4:4:4和4:2:0这种色彩抽样会损失信息，根据上文可知，解码算法提供了很多模式，压缩的程度也随模式、程度的选择等有所不同，信息的损失程度亦如此。而衡量一个视频通过编码损失的程度的参数就是码率，码率其实就是比特率（跟网络传输中的是一样的，不过这是在播放视频），也就是每秒的视频（很多张图片）一种多少数据。实际的数据少，说明压缩的程度高，即信息损失多，也就是会更不清楚。

### 码率和其他参数对画质的影响
在其他因素相同的情况下，码率高意味着编码算法对视频压缩程度低，画质一定是相对更高的。但现实就是哪有这么多相同的情况，so，画质，在视频源这一方面，都被哪些因素影响？（注意定语，也就是此时默认：之前讲过的显示器各种参数、操作系统色彩管理、解码器等不会成为观看视频的短板）
1. **编码格式**：比如同样的原始视频素材，同样的机器，同样的软件，最终导出的码率、分辨率都一样，但是唯一的区别就是一个是H265编码，另一个是H264编码，那么H265的最终视频看起来的画质极大概率高于H264，这是其算法原理导致的。
2. **分辨率**：分辨率和码率啥关系？到底哪个对画质的影响大？等等的问题是在码率相关问题中被问的最多的。因为有些极低码率的4k视频甚至看起来还不如高码率的1080p，一度怀疑这是4k视频吗？这种情况是存在的。这时4k视频只是理论上的4k视频，但是这4k的像素很多都是“错误”的，因为在编码是原始的像素信息都丢失了，都靠“预测”算法还原出来的，所以看起来会模糊，尤其是物体边缘（因为像素突变，帧内编码不好预测），更模糊的是运动快或者随机运动的对象，因为这时帧间编码对它的预测也不准了，反而这时候如果突然有相对静止的视频画面时，极低码率的4k视频画质可能又会反超1080p（预测又准了起来）。所以，影响都很大，且是动态变化的，当一个不是短板的时候，另一个就会成为短板，影响就相对变大，所以这个码率时根据分辨率、编码格式、视频中是否有大量不可预测的镜头等众多因素确定的，如果高了就是浪费，低了就让码率成为短板，画质就相对糊了。
3. 其他：还有一些其他因素：比如帧率也是，因为高帧率意味着需要更高码率（原理太过简单不说了）；比如软件、硬件的编码器也是，因为虽然标准规定了大致的算法流程，但是软件和硬件是具体的实现方式，其实还是会一定程度影响最终结果。就不一一列举了。

# 4播放器
操作系统一般都会内置很多软件，比如图片、音乐播放器等，同样也有视频播放器，但不知为何，所有主流操作系统的内置视频播放器都shit一样，各种兼容性问题，各种基本的快进等功能都难用到怀疑人生。我不知道为何会出现这种现象，因为很多其他系统软件还是做的不错的。anyway，我们目前看本地视频都只能用第三方播放器，比如mac的inna、windows的potplayer等。问题就在这里，第三方软件调用系统接口总会遇到各种各样的问题，到2024年还是无法做到开箱即用的4K HDR P3的视频播放，mac由于色彩管理和显示器素质高的优势，还是要领先windows不少的。

## 编码格式与封装格式


# 5拍摄&剪辑



